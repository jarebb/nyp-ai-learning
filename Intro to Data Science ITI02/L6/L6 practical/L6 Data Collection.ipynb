{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L6 Data Collection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"df4ilIImv17W"},"source":["# L6 Data Collection\n","\n","In this practical, you learn \n","* how to scrap web pages using Scrapy and extract the content using XPath, regex, and CSSSelector with LXML \n"]},{"cell_type":"markdown","metadata":{"id":"aaHVK950v-pD"},"source":["Useful Tools for Regex, XPath and CSSSelector development\n","Crawling and extraction rely heavily on the usage of XPath, and CSS Selector. However developing these patterns from scratch might be challenging, you might find some of the following tools useful.\n","\n","XPath Wizard\n","https://chrome.google.com/webstore/detail/xpath-helper-wizard/jadhpggafkbmpdpmpgigopmodldgfcki?hl=en\n","\n","Selector Gadget\n","https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en"]},{"cell_type":"markdown","metadata":{"id":"xgAzCSd_wa5q"},"source":["## Focused crawl\n","\n","First let's consider a simple crawler which crawl a quotes website using focused crawl strategy. \n","[quotes.toscrape.com](http://quotes.toscrape.com/)\n","\n","\n","Suppose that by navigating the website, we are able to guess the list of pages is in the shape of\n","`http://quotes.toscrape.com/page/1/`,\n","`http://quotes.toscrape.com/page/2/`,\n","\n","The quotes.toscrape example is inspired by [https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html ]. \n","\n","Recall from the lecture note that a focused crawl behaves as follows,\n","\n","1. For each URL `u` in the list of seed URLs,\n","     1. extract the needed content from `u`. \n","\n","We can use a function or just hard coding to generate the sequence of start / seed URLs.\n","\n","Note that a focused crawl does not follow links in the pages. We get a page from the list, and extract the needed content. \n","\n","\n","First of all we need to define some writer classes, which help to debug or save the output of the extract. \n","\n"," * ```ConsoleWriterPipeline``` receives the extract result from the spider and prints out the content. \n"," * ```JsonWriterPipeline``` receives the extract result from the spider and appends them into a JSON Line file, (each line is a json)"]},{"cell_type":"code","metadata":{"id":"gGZqPfi-G0VG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ir-S6X55Tt9U"},"source":["data_dir_path='/content/drive/My Drive/Data/DS6/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YoFs6LJCwcvH"},"source":["import lxml.etree\n","\n","import json\n","\n","# receives the extract result from the spider and prints out the content\n","class ConsoleWriterPipeline(object):\n","    def open_spider(self, spider):\n","        None\n","    def close_spdier(self, spider):\n","        None\n","    \n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        print(line)\n","        return item\n","    \n","# receives the extract result from the spider and appends them into a JSON Line file, (each line is a json)\n","class JsonWriterPipeline(object):\n","    def open_spider(self, spider):\n","        self.file = open(data_dir_path+'result.json', 'w')\n","\n","    def close_spider(self, spider):\n","        print('JSON File Generated')\n","        self.file.close()\n","\n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        self.file.write(line)\n","        return item"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o8CDdwiKxwWf"},"source":["Next we define our spider, `QuoteSpider` is a focused spider. \n","\n","It reads a list of URLs and calls `parse()` for each page (`response`) given by the link. \n","\n","Note that for each link, we find multiple quotes. Hence, in `parse()` we use a CSS selector to retrieve the list of all `div` elements that containing the quotes, one quote per element. \n","\n","The `yield` statement constructs the result JSON object that will be consumed by the downstream writer, in this case we use `ConsoleWriterPipeline`. \n","\n","\n","![image](https://drive.google.com/uc?id=13tZtcspz_KFJecaNMs8ZQt3RnYz3qHWI)"]},{"cell_type":"markdown","metadata":{"id":"GcFASGZ-pujZ"},"source":["\n","\n","```\n","for quote in response.css('div.quote'):\n","            yield {\n","                'text': quote.css('span.text::text').get(),\n","                'author': quote.css('span small::text').get(),\n","                'tags': quote.css('div.tags a.tag::text').getall(),\n","            }\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"u4poYenjy10j"},"source":["!pip install scrapy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnxDT6Jqp_o4"},"source":["import logging\n","import scrapy\n","from scrapy.crawler import CrawlerProcess\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","    start_urls = [\n","        'http://quotes.toscrape.com/page/1/',\n","        'http://quotes.toscrape.com/page/2/',\n","    ]\n","    custom_settings = {\n","        'LOG_LEVEL': logging.WARNING,                            # Default : Debug\n","        'ITEM_PIPELINES': {'__main__.ConsoleWriterPipeline': 1} # Used for pipeline\n","    }\n","    \n","    def parse(self, response):\n","        #Add code\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnQZIlwnyWvJ"},"source":["In the following, we create a process which will start the crawler. By uncommenting and running the below code, we perform the focused crawl the web site. The result will be printed in the output sessoin. In case it does not stop. You consider click the \"Block Square\" button below the menu bar to stop the kernel. \n","\n","*Note* In case you hit the `ReactorNotRestartable: ` error, you should comment away another crawler processes in this note book and restart the kernel."]},{"cell_type":"markdown","metadata":{"id":"m3gX1T7xn1aR"},"source":["User Agent is the runner that we use to execute the crawling process.\n","\n","For more details, refer to https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent"]},{"cell_type":"code","metadata":{"id":"oFJj4uQIyX6A"},"source":["# uncomment me and run\n","# '''\n","quotes_crawler_process = CrawlerProcess({\n","    'USER_AGENT': 'Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n","})\n","\n","quotes_crawler_process.crawl(QuotesSpider)\n","quotes_crawler_process.start()\n","# '''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNkofjFazqk7"},"source":["## Exercise\n","\n","When you are happy with result, you may modify the `QuotesSpider` class to use `JSONWriterPipeline` to save the result in a file."]},{"cell_type":"markdown","metadata":{"id":"WDLRjSNozsIe"},"source":["##  Question\n","\n","Manually getting the list of input URLs for a focused crawl could be challenging? Is there anyway to automate it?   \n","\n","In the following example, we are going to BBC and get the headline and introduction from all the pages that can be access the landing URL.\n"]},{"cell_type":"markdown","metadata":{"id":"UE_9HaQ_oZtg"},"source":["## General Crawl - News Crawler\n","\n","Restart Runtime to avoid ReactorNotRestartable error.\n"]},{"cell_type":"markdown","metadata":{"id":"jIAozfpi_4kA"},"source":["At the http://www.bbc.co.uk/news/technology/ page parse the articles.\n","\n","Get each article text for headline and introduction.\n","\n","Setup the parsing result either console or json file. \n"]},{"cell_type":"code","metadata":{"id":"4Ny9nxtbV-qs"},"source":["import lxml.etree\n","\n","import json\n","    \n","class ConsoleWriterPipeline(object):\n","    def open_spider(self, spider):\n","        None\n","    def close_spider(self, spider):\n","        None\n","    \n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        print(line)\n","        return item\n","    \n","class JsonWriterPipeline(object):\n","    def open_spider(self, spider):\n","        self.file = open(data_dir_path+'newsresult.json', 'w')\n","\n","    def close_spider(self, spider):\n","        print('JSON File Generated')\n","        self.file.close()\n","\n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        self.file.write(line)\n","        return item "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p0CzpwL5AzJh"},"source":["Define the start Url\n","\n","\n","```\n","\"http://www.bbc.co.uk/news/technology/\"\n","```\n","\n","Set the rule for the parsing in the URL\n","\n","```\n","Rule(LinkExtractor(allow=['/technology-\\d+'])\n","```\n","\n","Parsing function to extract each article headline and introduction\n","\n","```\n","    story = NewsItem()\n","    story['headline'] = response.xpath('//head/title/text()').get()\n","    story['intro'] = response.xpath('//p/b/text()').get()\n","    \n","    yield {\n","        \"headline\":story['headline'],\n","        \"intro\":story['intro']\n","        }\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nnHuxPL9CceX"},"source":["Tag for the Headline\n","\n","![image](https://drive.google.com/uc?id=19r3WBTOHK3tj6k55I7h-tXWOegc3mQvv)\n","\n","Tag for the introduction\n","\n","![image](https://drive.google.com/uc?id=1lZnqIqx64sOuaHPGq8sQL8nTemJjoCWM)"]},{"cell_type":"code","metadata":{"id":"--AuarktA0Ze"},"source":["import logging\n","import scrapy\n","from scrapy.spiders import Rule, CrawlSpider\n","from scrapy.linkextractors import LinkExtractor\n","\n","class NewsItem(scrapy.Item):\n","  # define the fields for your item here like:\n","  headline = scrapy.Field()\n","  intro = scrapy.Field()\n","\n","class NewsSpider(CrawlSpider):\n","  name = \"bbcnews\"\n","  allowed_domains = [\"bbc.co.uk\"]\n","  start_urls = [#Add code ,]\n","  custom_settings = {\n","      'LOG_LEVEL': logging.WARNING,\n","      'ITEM_PIPELINES': {'__main__.ConsoleWriterPipeline': 1} # Used for pipeline 1\n","      }\n","  rules = [#Add code , 'parse_story')]\n","\n","  def parse_story(self, response):\n","    story = NewsItem()\n","    story['headline'] = response.xpath(#Add code ).get()\n","    story['intro'] = response.xpath(#Add code ).get()\n","    yield {\n","        \"headline\":story['headline'],\n","        \"intro\":story['intro']\n","        }\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zhf5FBJRWAcJ"},"source":["from scrapy.crawler import CrawlerProcess\n","\n","hgw_crawler_process = CrawlerProcess({\n","    'USER_AGENT': 'Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n","})\n","\n","hgw_crawler_process.crawl(NewsSpider)\n","hgw_crawler_process.start()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7X4oGeNCz2U1"},"source":["## General Crawl - Book Crawler\n","\n","\n","A general crawler may have only one start URL, and typically two rules. It starts by added the start URL to its URL queue.\n","it repeats the following until the URL queue is empty.\n"," 1. get a URL from the from the URL queue,\n","     1. rule 1. when a target URL is loaded, extract it.\n","     1. rule 2. when a non-target URL is loaded and add all (new) links in the page the URL queue.\n"," 1. remove the URL from the URL queue.\n"," \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"co0wz4iaDyL3"},"source":["Goto http://books.toscrape.com.\n","\n","Extract each book title, price and stock\n","\n","\n","![image](https://drive.google.com/uc?id=1Z5pnlu47Wjc7Nl11EBRdM5E71QCPuuua)\n","\n","\n","\n","\n","```\n","yield {\n","            'title': response.css('.product_main h1::text').get(),\n","            'price': response.css('.product_main p.price_color::text').re_first('£(.*)'),\n","            'stock': int(\n","                ''.join(\n","                    response.css('.product_main .instock.availability ::text').re('(\\d+)')\n","                )\n","            ),\n","        }\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"GgYjbbR75Sma"},"source":["import lxml.etree\n","\n","import json\n","    \n","class ConsoleWriterPipeline(object):\n","    def open_spider(self, spider):\n","        None\n","    def close_spdier(self, spider):\n","        None\n","    \n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        print(line)\n","        return item\n","    \n","class JsonWriterPipeline(object):\n","    def open_spider(self, spider):\n","        self.file = open(data_dir_path+'bookresult.json', 'w')\n","\n","    def close_spider(self, spider):\n","        print('JSON File Generated')\n","        self.file.close()\n","\n","    def process_item(self, item, spider):\n","        line = json.dumps(dict(item)) + \"\\n\"\n","        self.file.write(line)\n","        return item "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yE-llcU4IFYF"},"source":["import logging\n","import scrapy\n","from scrapy.spiders import CrawlSpider, Rule\n","from scrapy.linkextractors import LinkExtractor\n","\n","\n","class BooksCrawlSpider(CrawlSpider):\n","    name = 'books-crawlspider'\n","    allowed_domains = ['toscrape.com']\n","    start_urls = ['http://books.toscrape.com']\n","    custom_settings = {\n","      'LOG_LEVEL': logging.WARNING,\n","      'ITEM_PIPELINES': {'__main__.ConsoleWriterPipeline': 1}#, # Used for pipeline 1\n","      }\n","    rules = [\n","        Rule(\n","            LinkExtractor(allow=('/catalogue/page-\\d+.html')),follow=True\n","        ),\n","        Rule(\n","             LinkExtractor(deny=('/category/books', '.com/index.html')),callback='parse_book_page',\n","            follow=True\n","        ),\n","    ]\n","\n","    def parse_book_page(self, response):\n","        #Add code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpIHzl1OZkXg"},"source":["#  uncomment me and run\n","from scrapy.crawler import CrawlerProcess\n","\n","hgw_crawler_process = CrawlerProcess({\n","    'USER_AGENT': 'Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n","})\n","\n","hgw_crawler_process.crawl(BooksCrawlSpider)\n","hgw_crawler_process.start()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov4TpsJmpHL5"},"source":["# RestFul API"]},{"cell_type":"markdown","metadata":{"id":"YLAMbskPJsX0"},"source":["Let's use API from data.gov.sg to check the PSI readings.\n","\n","https://api.data.gov.sg/v1/environment/psi\n","\n","When you click on the above link, you see that the data return is in json format.\n","\n","With reference to lecture slide 34 and 35, let's extract the PSI 24-hourly reading."]},{"cell_type":"code","metadata":{"id":"LMkW5qziRBDA","executionInfo":{"status":"ok","timestamp":1621505682548,"user_tz":-480,"elapsed":1596,"user":{"displayName":"Jasmine Ng","photoUrl":"","userId":"03140874638654819624"}}},"source":["# importing the requests library\n","import requests \n","\n","# api-endpoint\n","# add code here\n","\n","# sending get request and saving the response as response object\n","# add code here\n","\n","# extracting data in json format\n","# add code here"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZY-YLmZkRG-z"},"source":["It is quite difficult to view the json data from the notebook. We can make use of online JSON Viewer such as http://jsonviewer.stack.hu/ to help us.\n","\n","From the json viewer, look for psi_twenty_four_hourly which is the data that we want to display.\n","\n","Note that the sample in lecture is reading pm25 one hourly reading, but now we want PSI 24-hourly reading."]},{"cell_type":"code","metadata":{"id":"p9jL5DPERHZG"},"source":["# extracting PSI24 readings\n","# add code here"],"execution_count":null,"outputs":[]}]}